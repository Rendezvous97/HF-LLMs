{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env.local\n",
    "load_dotenv('.env.local')\n",
    "\n",
    "# Now you can access your token\n",
    "hf_token = os.getenv(\"HF_TOKEN\")  # Replace with your variable name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ELI5 dataset\n",
    "\n",
    "Start by loading the first 5000 examples from the ELI5-Category dataset with the ðŸ¤— Datasets library. Thisâ€™ll give you a chance to experiment and make sure everything works before spending more time training on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "eli5 = load_dataset(\"sentence-transformers/eli5\", split=\"train[:5000]\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = eli5.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"How is it someone can do 100 squats but can't run but a few miles?\",\n",
       " 'answer': \"ELI5: bulldozers can lift tons of weight but can't go fast. They are built for one thing and they do other things not as well. Race cars can go fast but can't lift tons.\"}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "\n",
    "The next step is to load a DistilGPT2 tokenizer to process the text subfield:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruct(example):\n",
    "    prompt = f\"### Instruction:\\n{example['question']}\\n\\n### Response:\\n{example['answer']}\"\n",
    "    return {\"text\": prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d6c7a5fc664591855936de4ce5a51a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512059f3867b4f7db6753cda0f797dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = eli5.map(format_instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': \"Why didn't the U.S. include the release of prisoners in the new nuclear deal with Iran\",\n",
       " 'answer': 'Because that would have given leverage to Iran. By taking more prisoners and negotiating for their release too, they could have tried to extract more favorable terms in other parts of the agreement. By not including prisoner release as part of the negotiations, the U.S. removed that potential negotiating card for them to play, forcing them to negotiate only on nuclear research and sanctions and nothing else.',\n",
       " 'text': \"### Instruction:\\nWhy didn't the U.S. include the release of prisoners in the new nuclear deal with Iran\\n\\n### Response:\\nBecause that would have given leverage to Iran. By taking more prisoners and negotiating for their release too, they could have tried to extract more favorable terms in other parts of the agreement. By not including prisoner release as part of the negotiations, the U.S. removed that potential negotiating card for them to play, forcing them to negotiate only on nuclear research and sanctions and nothing else.\"}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token  # required for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efb45b6f05945f49264a9a8c39318b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6b15bff5f704582ace6a08abdd5a581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, remove_columns=dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling, AutoModelForCausalLM\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from accelerate>=0.26.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from accelerate>=0.26.0) (24.2)\n",
      "Requirement already satisfied: psutil in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from accelerate>=0.26.0) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from accelerate>=0.26.0) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from accelerate>=0.26.0) (2.2.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.31.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from accelerate>=0.26.0) (0.5.3)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.3.0)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from huggingface_hub>=0.21.0->accelerate>=0.26.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from torch>=2.0.0->accelerate>=0.26.0) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from jinja2->torch>=2.0.0->accelerate>=0.26.0) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from requests->huggingface_hub>=0.21.0->accelerate>=0.26.0) (2025.6.15)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages (from sympy->torch>=2.0.0->accelerate>=0.26.0) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install 'accelerate>=0.26.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sp/fmx5ldl12kqc2prls4b88nqm0000gn/T/ipykernel_16176/1930029035.py:13: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 1:03:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.547200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.430300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>3.341000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.308200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.254900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3000, training_loss=3.3606617024739585, metrics={'train_runtime': 3823.511, 'train_samples_per_second': 3.138, 'train_steps_per_second': 0.785, 'total_flos': 1567780503552000.0, 'train_loss': 3.3606617024739585, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./instruct-chatbot\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/opt/miniconda3/envs/py3-12-LLM/lib/python3.12/site-packages/transformers/pytorch_utils.py:328: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_elements = torch.tensor(test_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction:\n",
      "How do airplanes fly?\n",
      "\n",
      "### Response:\n",
      "Airplane is the same type of plane as the engine, and it's the same engine that holds it and the engine produces a lot of power. The plane is the same size as the engine. This makes it a little more accurate. In a plane, it is an engine that can have a lot of torque, and the engine is the same size. In a plane, it has the same weight, but it has the same engine. The engine is the same size. The engine has the\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "prompt = \"### Instruction:\\nHow do airplanes fly?\\n\\n### Response:\\n\"\n",
    "output = pipe(prompt, max_new_tokens=100, do_sample=True, top_p=0.95, temperature=0.7)\n",
    "\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-12-LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
