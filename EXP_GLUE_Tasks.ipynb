{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š GLUE Benchmark â€” Task Overview\n",
    "\n",
    "GLUE includes 9 English language understanding tasks, each with a different structure and purpose. These tasks cover:\n",
    "\t\n",
    "\tâ€¢\tSingle-sentence classification\n",
    "\tâ€¢\tSentence pair classification\n",
    "\tâ€¢\tSemantic similarity (regression)\n",
    "\tâ€¢\tNatural language inference (NLI)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”¹ 1. CoLA (Corpus of Linguistic Acceptability)\n",
    "\n",
    "- **Task Type**: Single-sentence classification\n",
    "- **Goal**: Judge grammatical acceptability\n",
    "- **Input**: One sentence\n",
    "- **Label**: \n",
    "  - 1 = grammatically acceptable  \n",
    "  - 0 = unacceptable\n",
    "- **Example**:  \n",
    "  `\"The boy is sleeping.\" â†’ 1`\n",
    "- **Dataset Size**: ~8.5k train, 1k dev\n",
    "- **Source**: Linguistic literature (Warstadt et al., 2018)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. SST-2 (Stanford Sentiment Treebank)\n",
    "\n",
    "- **Task Type**: Single-sentence classification\n",
    "- **Goal**: Predict sentiment (positive/negative)\n",
    "- **Input**: One sentence\n",
    "- **Label**: \n",
    "  - 1 = positive  \n",
    "  - 0 = negative\n",
    "- **Example**:  \n",
    "  `\"A delightfully quirky film.\" â†’ 1`\n",
    "- **Dataset Size**: ~67k train, 872 dev\n",
    "- **Source**: Stanford Sentiment Treebank\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. MRPC (Microsoft Research Paraphrase Corpus)\n",
    "\n",
    "- **Task Type**: Sentence pair classification\n",
    "- **Goal**: Determine if two sentences are paraphrases\n",
    "- **Input**: Sentence1, Sentence2\n",
    "- **Label**: \n",
    "  - 1 = paraphrase  \n",
    "  - 0 = not paraphrase\n",
    "- **Example**:  \n",
    "  `\"He ran the company.\" vs \"He managed the business.\" â†’ 1`\n",
    "- **Dataset Size**: ~3.7k train, 408 dev\n",
    "- **Source**: Microsoft Research\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 4. QQP (Quora Question Pairs)\n",
    "\n",
    "- **Task Type**: Sentence pair classification\n",
    "- **Goal**: Identify if two questions have the same meaning\n",
    "- **Input**: Question1, Question2\n",
    "- **Label**: \n",
    "  - 1 = duplicate  \n",
    "  - 0 = not duplicate\n",
    "- **Example**:  \n",
    "  `\"What is AI?\" vs \"What is artificial intelligence?\" â†’ 1`\n",
    "- **Dataset Size**: ~364k train, 40k dev\n",
    "- **Source**: Quora\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 5. STS-B (Semantic Textual Similarity Benchmark)\n",
    "\n",
    "- **Task Type**: Sentence pair regression\n",
    "- **Goal**: Rate how similar two sentences are (scale: 0 to 5)\n",
    "- **Input**: Sentence1, Sentence2\n",
    "- **Label**: Float between 0.0 and 5.0\n",
    "- **Example**:  \n",
    "  `\"A man is playing guitar.\" vs \"Someone is playing an instrument.\" â†’ 4.2`\n",
    "- **Dataset Size**: ~5.7k train, 1.5k dev\n",
    "- **Source**: Various MTurk datasets\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 6. MNLI (Multi-Genre Natural Language Inference)\n",
    "\n",
    "- **Task Type**: Sentence pair classification (NLI)\n",
    "- **Goal**: Determine if the hypothesis is entailed by, contradicts, or is neutral with respect to the premise\n",
    "- **Input**: Premise, Hypothesis\n",
    "- **Label**: \n",
    "  - 0 = entailment  \n",
    "  - 1 = neutral  \n",
    "  - 2 = contradiction\n",
    "- **Example**:  \n",
    "  `Premise: \"He ordered pizza.\"  \n",
    "   Hypothesis: \"He got food.\" â†’ 0 (entailment)`\n",
    "- **Dataset Size**: ~393k train, 20k dev\n",
    "- **Source**: Multi-genre corpora (spoken, fiction, government, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 7. QNLI (Question Natural Language Inference)\n",
    "\n",
    "- **Task Type**: Sentence pair classification (QA reformulated as NLI)\n",
    "- **Goal**: Determine if a context sentence contains the answer to a given question\n",
    "- **Input**: Question, Sentence\n",
    "- **Label**: \n",
    "  - 1 = answerable (entailment)  \n",
    "  - 0 = not answerable (neutral)\n",
    "- **Dataset Size**: ~105k train, 5.4k dev\n",
    "- **Source**: SQuAD reformatted\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 8. RTE (Recognizing Textual Entailment)\n",
    "\n",
    "- **Task Type**: Sentence pair classification (NLI)\n",
    "- **Goal**: Determine if premise entails the hypothesis\n",
    "- **Input**: Premise, Hypothesis\n",
    "- **Label**: \n",
    "  - 1 = entailment  \n",
    "  - 0 = not entailment\n",
    "- **Dataset Size**: ~2.5k train, 277 dev\n",
    "- **Source**: RTE Challenges (1â€“4)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 9. WNLI (Winograd NLI)\n",
    "\n",
    "- **Task Type**: Sentence pair classification (NLI/coreference)\n",
    "- **Goal**: Determine if the hypothesis is entailed based on coreference resolution\n",
    "- **Input**: Sentence1, Sentence2\n",
    "- **Label**: \n",
    "  - 1 = entailment  \n",
    "  - 0 = not entailment\n",
    "- **Dataset Size**: 634 train, 146 dev\n",
    "- **Note**: Known to be adversarial; some models perform worse than chance\n",
    "- **Source**: Winograd Schema Challenge items\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… GLUE Summary Table\n",
    "\n",
    "| Task    | Input            | Label Type       | Task Type               | Goal                                |\n",
    "|---------|------------------|------------------|--------------------------|-------------------------------------|\n",
    "| CoLA    | Sentence          | 0/1              | Single-sentence classification | Grammatical acceptability           |\n",
    "| SST-2   | Sentence          | 0/1              | Sentiment classification | Sentiment polarity                  |\n",
    "| MRPC    | Sentence pair     | 0/1              | Paraphrase classification | Are the sentences paraphrases?     |\n",
    "| QQP     | Question pair     | 0/1              | Duplicate question classification | Are questions semantically equivalent? |\n",
    "| STS-B   | Sentence pair     | Float [0.0â€“5.0]  | Semantic similarity regression | Degree of semantic similarity       |\n",
    "| MNLI    | Sentence pair     | 0/1/2            | Natural Language Inference | Entailment, contradiction, or neutral |\n",
    "| QNLI    | Question + sentence | 0/1            | QA-style entailment      | Does sentence answer the question? |\n",
    "| RTE     | Sentence pair     | 0/1              | Textual entailment       | Is the hypothesis entailed?        |\n",
    "| WNLI    | Sentence pair     | 0/1              | Coreference-based NLI    | Entailment based on coreference    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_glue(task_name:str, tokenizer=None, checkpoint='bert-base-uncased'):\n",
    "    # Define the input fields for each GLUE task\n",
    "    sentence_keys = {\n",
    "        \"cola\": (\"sentence\", None),\n",
    "        \"sst2\": (\"sentence\", None),\n",
    "        \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "        \"qqp\": (\"question1\", \"question2\"),\n",
    "        \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "        \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "        \"qnli\": (\"question\", \"sentence\"),\n",
    "        \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "        \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "    }\n",
    "\n",
    "    if tokenizer is None:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    raw_dataset = load_dataset('glue', task_name)\n",
    "\n",
    "    key1, key2 = sentence_keys[task_name.lower()]\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        if key2 == None:\n",
    "            return tokenizer(examples[key1], truncation=True)\n",
    "        else:\n",
    "            return tokenizer(examples[key1], examples[key2], truncation=True)\n",
    "\n",
    "    tokenized_datasets = raw_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # Handle label format (float for STS-B, int for others)\n",
    "    def format_labels(example):\n",
    "        if task_name.lower() == \"stsb\":\n",
    "            example[\"label\"] = float(example[\"label\"])\n",
    "        else:\n",
    "            example[\"label\"] = int(example[\"label\"])\n",
    "        return example\n",
    "\n",
    "    tokenized_datasets = tokenized_datasets.map(format_labels, batched=False)\n",
    "\n",
    "    non_tensor_columns = raw_dataset[\"train\"].column_names\n",
    "    tokenized_datasets = tokenized_datasets.remove_columns(\n",
    "        [col for col in non_tensor_columns if col not in [\"label\"]]\n",
    "    )\n",
    "\n",
    "    # Set format for PyTorch\n",
    "    tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "    # Create a dynamic padding collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    return tokenized_datasets, data_collator, \"label\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57fffa9b22fa49d08f75427d47488cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/277 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "610a41a77716409a8ef2ab06b8d0c0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d959fbeaa464900a47c24f300735b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/277 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deb40573b8704331af93666e4d49aa39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': ClassLabel(names=['entailment', 'not_entailment'], id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets, data_collator, label_column = preprocess_glue(\"rte\")\n",
    "print(tokenized_datasets[\"train\"].features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-12-LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
